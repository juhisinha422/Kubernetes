
---

# ğŸ“˜ **Kubeadm HA Cluster on AWS using Terraform**

This project provisions a full **Highly Available Kubernetes cluster** using **Terraform**, **EC2**, **HAProxy**, **custom VPC**, and **modular Infrastructure as Code (IaC)**.

The environment includes:

* **Custom Production-Grade VPC**
* **Security Group Module**
* **EC2 Master Nodes** (kubeadm control plane)
* **EC2 Worker Nodes**
* **HAProxy Load Balancer** (API server endpoint)
* **IAM Roles & SSM access**
* Fully modular, reusable, and scalable infrastructure

---

# ğŸ“‚ **Project Structure**

```
.
â”œâ”€â”€ main.tf                     # Root module using all child modules
â”œâ”€â”€ variables.tf                # Root variables
â”œâ”€â”€ outputs.tf                  # Root outputs
â”œâ”€â”€ providers.tf                # AWS provider config
â”œâ”€â”€ versions.tf                 # Terraform + provider versions
â”‚
â”œâ”€â”€ modules/
â”‚   â”œâ”€â”€ vpc/
â”‚   â”‚   â”œâ”€â”€ main.tf             # VPC, subnets, IGW, RTs
â”‚   â”‚   â”œâ”€â”€ provider.tf
â”‚   â”‚   â””â”€â”€ variables.tf
â”‚   â”‚
â”‚   â”œâ”€â”€ securtiy-groups/
â”‚   â”‚   â”œâ”€â”€ main.tf             # Master/Worker/LB security groups
â”‚   â”‚   â”œâ”€â”€ variables.tf
â”‚   â”‚   â””â”€â”€ outputs.tf
â”‚   â”‚
â”‚   â”œâ”€â”€ ec2-instance/
â”‚   â”‚   â”œâ”€â”€ main.tf             # EC2 launch module for masters/workers
â”‚   â”‚   â”œâ”€â”€ variables.tf
â”‚   â”‚   â”œâ”€â”€ outputs.tf
â”‚   â”‚   â””â”€â”€ userdata.tpl        # Kubeadm install/bootstrap script
â”‚   â”‚
â”‚   â””â”€â”€ haproxy/
â”‚       â”œâ”€â”€ main.tf             # HAProxy instance (API LB)
â”‚       â”œâ”€â”€ variables.tf
â”‚       â”œâ”€â”€ outputs.tf
â”‚       â””â”€â”€ userdata.tpl        # HAProxy configuration template
```

---

# ğŸš€ **Architecture Overview**

## 1ï¸âƒ£ VPC Module

Creates production-grade networking:

* **VPC CIDR** (10.0.0.0/16)
* **3 Public Subnets** + **3 Private Subnets**
* **Internet Gateway**
* **Public Route Table**
* No NAT Gateway (early phase â†’ cost saving)
* Outputs:

  * `vpc_id`
  * `public_subnets`
  * `private_subnets`

This ensures all nodes are deployed into **our own isolated network**, not the AWS default VPC.

---

## 2ï¸âƒ£ Security Group Module

Creates dedicated security groups for:

### ğŸ”¹ Master SG

* 6443 â†’ kube-api server
* 10259 â†’ scheduler
* 10257 â†’ controller-manager
* 2379â€“2381 â†’ etcd
* 10250 â†’ kubelet
* SSH â†’ (allowed CIDR)

### ğŸ”¹ Worker SG

* 10250 â†’ kubelet
* 10256 â†’ kube-proxy
* 30000â€“32767 â†’ NodePort
* SSH

### ğŸ”¹ HAProxy SG

* 6443 open (API LB)
* SSH

Outputs:

* `master_sg_id`
* `worker_sg_id`
* `kube_api_lb_sg_id`

---

## 3ï¸âƒ£ EC2 Instance Module

Used to provision:

* **Master nodes**
* **Worker nodes**

Includes:

* AMI
* Instance type
* Subnet
* SSM IAM Role (for parameter store)
* Userdata template that:

  * installs container runtime
  * installs kubeadm, kubelet, kubectl
  * bootstraps cluster or joins worker

---

## 4ï¸âƒ£ HAProxy Module

Provisions 1 EC2 instance that acts as a **Load Balancer** for the kube-apiserver.

HAProxy forwards:

```
6443 â†’ master nodes
```

Control plane endpoint:

```
https://<haproxy-private-ip>:6443
```

Used in kubeadm init.

---

# ğŸ”„ **Cluster Workflow**

### Step 1 â€” VPC module runs first

Creates:

* VPC
* Subnets
* IGW
* Route tables

### Step 2 â€” Security groups

Created inside the VPC using outputs from Step 1.

### Step 3 â€” Master nodes

EC2 "masters" are created:

* 3 masters â†’ 1 in each public subnet
* Uses `master_sg_id`
* Bootstrap via kubeadm

### Step 4 â€” Wait for masters

A `null_resource` enforces completion before HAProxy.

### Step 5 â€” HAProxy

HAProxy is deployed:

* Gets IPs of master nodes
* Generates dynamic HAProxy config
* Starts forwarding Kubernetes traffic

### Step 6 â€” Worker nodes

Workers join the cluster automatically using:

* SSM parameter store tokens
* kubeadm join command
* Worker SG

---

# ğŸ“¥ **How to Deploy**

### 1. Initialize Terraform

```bash
terraform init
```

### 2. Validate configuration

```bash
terraform validate
```

### 3. Format files (optional)

```bash
terraform fmt
```

### 4. View deployment plan

```bash
terraform plan
```

### 5. Apply infrastructure

```bash
terraform apply -auto-approve
```

Terraform will create:

âœ” VPC
âœ” Subnets
âœ” Security groups
âœ” IAM roles
âœ” 3 master nodes
âœ” HAProxy
âœ” Worker nodes

---

# ğŸ“¤ **Destroy environment**

```bash
terraform destroy
```

Make sure to delete kubeadm configs stored in SSM parameter store if needed.

---

# ğŸ”§ **Configuration (terraform.tfvars)**

Example:

```hcl
cluster_name          = "kubeadm-ha"
aws_region            = "ap-south-1"
masters_count         = 3
workers_count         = 3
ubuntu_ami            = "ami-xxxxxxx"
master_instance_type  = "t3.medium"
worker_instance_type  = "t3.small"
haproxy_instance_type = "t3.micro"
ssh_key_name          = "mykey"

vpc_cidr_range = "10.0.0.0/16"

subnet_azs = [
  "ap-south-1a",
  "ap-south-1b",
  "ap-south-1c"
]

public_subnet_cidrs = [
  "10.0.1.0/24",
  "10.0.2.0/24",
  "10.0.3.0/24"
]

private_subnet_cidrs = [
  "10.0.11.0/24",
  "10.0.12.0/24",
  "10.0.13.0/24"
]
```

---

# ğŸ§© **What This Automates**

âœ” Creates fully production-ready VPC
âœ” Deploys 3 master control plane nodes
âœ” Deploys HAProxy load balancer
âœ” Deploys any number of worker nodes
âœ” Automates join tokens via SSM
âœ” Uses modular architecture for reusability
âœ” Zero manual provisioning

---

# ğŸ“Œ **Why This Approach?**

This project follows DevOps best practices:

* **Infrastructure as Code** (Terraform)
* **Modular Design** â†’ reusable across different clusters
* **Highly Available Control Plane**
* **Separation of concerns**:

  * network module
  * security group module
  * EC2 module
  * HAProxy module
* **Parameter Store for automation**
* **No default VPC usage**
* **Scalable across AZs**


---
